{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Baby Mood Prediction via Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific Question: Can prediction algorithms be utilized to predict baby moods, such as, happiness, sadness, fright, disgust, hunger, thirst, and sleepiness from neural frequencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babies cry a lot and for many different reasons, sometimes we know what they want, but other times we are left in the dust. It would be nice to know how they feel and since we cannot talk with them, it would be even better if we could look inside their brains and just know what they want! That is where my prediction algorithm and I come in. We know that whenever you, a baby, or I want something there is neural activity taking place. Electrodes and EEGs are devices and test that allow us to examine electrical charges and activity in specific regions of the brain. With the use of electrodes this neural activity can be monitored and can grasp this neural activity in the form of high gamma and low frequencies. These frequencies correspond with regions of the brain and respectively, with different feelings, emotions, and important to us, moods.\n",
    "\n",
    "For this experiment we are utilizing the MNIST database, this database consists of 70,000 black and white images of reconstructed baby brain complexes via machine learning algorithms that utilized the neural activation and respective frequency for specific brain region and complex reconstruction in the form of 28 x 28 pixelated images. The data on the frequencies utilized for reconstruction came from implanting electrodes directly onto the babies' brains and retrieving information based on their known moods. The data came out to 60,000 images used for training and 10,000 images used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Hypothesis: If babies' moods and their respective neural activation and frequency are correlated with specific parts of the brain, then we can utilize a prediction algorithm to examine reconstructed neural activity in the form of brain images to predict brain complexes and their respective mood association. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis was performed by clustering the data into known and unknown, given by their respective training and testing denotation. This data was fed into a Deep Neural Network prediction algorithm and propagated through to examine, measure, and increase prediction accuracy. We observe respective loss and prediction accuracy through each epoch (or pass through). We are distinctly examining the Convolution Neural Network's error, to illuminate error reduction desires we compare and contrast each epoch and different levels of complexity for the Deep Neural Network (CNN), both for a simple and complex model, respectively. \n",
    "\n",
    "To answer this scientific question as well as test my hypothesis I needed to locate a dataset that had collected neural frequencies from babies based on their moods and reconstructed brain parts and complexes based on those collected frequencies. Fortunately, this data (proof of concept) is part of the Keras library and can be imported directly from there once adding the Keras library to your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load the Packages: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras is an API or an application programming interface, this particular interface is used specifically in Python and creates a means for utilizing artificial neural networks. Keras handles weight calculation and simplifies layer input. It notably carries the dataset used here, MNIST.\n",
    "- Tensorflow is a library used for machine learning, it has tools, datasets, libraries, packages, and resources that help with deployment of Machine Learning. Keras has Tensorflow wrapped inside of it and is how Keras is able to function and compute on the back end. Notably, it is not directly imported because Keras runs over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are importing all the packages and subpackages that are necessary to run our Deep Neural Network\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load the Data, split it into Testing and Training, and Reshape it: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is loaded in, since the dataset is split into testing and training sets, we first need to classify the clusters in accordance to how they appear in the dataframe. The data needs to be reshaped via linear transformations by the samples, width, height, and channels. The data needs to be converted to 32 bit in order to allow it to be processed by our ram more easily. Similarly the data must be normalized, this ids done by dividing our test and training data by 225 in accordance to the max byte size value, doing this will help keep our data between 0 and 1, which is necessary for the inputs and outputs and for computing weights as they are passed through and propagated by the Network. We utilize our utilities in order to take the integers and make them binary, this is done with one hot and is specifically used for our y outputs. This is done as at the end of our network layer, the total layer is formed of 10 neurons that all add to 1 in value, the value with the highest number is the network's \"prediction neuron\" as I like to describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is loaded in with respect to its cluster as a part of the training or testing set, the x are the inputs and\n",
    "#the y values are the outputs\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# The data needs to be reshaped in the form of the samples, width, height, and channels. \n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
    "# The inputs are normalized so that they are integers between 0 and 1, this is doen by dividing by 255.\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# The outputs, the y values for train and test, the integers are made binary.\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define and Create the Simple Convolution Neural Network Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the architecture of our Neural Network. We define and create the model, whereby the sequential format is best for our particular model data. The sequential model is useful for our architecture as it allows us to build each layer individually, this is particularly useful for us as we are keeping this neural network concise and efficient. We are making our network a Convolution Neural Network as this form of neural network works well for analyzing images and is especially useful for the 28x28 image constructions passed through. Since we are using a sequential model, we can choose our activation desired for each layer. Relu refers to activations that pass weighted values only if they are positive, negative values will be fixed and passed as 0 (so no negative weights are passed), this aids in learning and prediction efficiency and performance. The softmax activation is used in our final layer to illustrate the networkâ€™s probability distribution and pushes the results to either 0 or 1. Other layers will have different activations and uses as will be annotated in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the Simple Convolution Neural Network model and architecture\n",
    "def baseline_model():\n",
    "\t# The model's architecture is build and layers, from input to output are added\n",
    "    # The sequential model is our model type as it allows for us to add each layer individually\n",
    "\tmodel = Sequential()\n",
    "    # First Convolution layer is added and is with respect to our inputs, we are using 32 convolution filters, and 5 kernel rows\n",
    "    # and columns. One input is 1 28x28 pixelated image. The activation relu is used here as described above.\n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    # This layer is used to filter and reduced the parameters as they are passed through the CNN.\n",
    "\tmodel.add(MaxPooling2D())\n",
    "    # Dropouts are added to prevent overfitting, a value of 1 means there will be no dropout and closer to 0 means no output.\n",
    "    # Here I elected to used 0.2 because this is our input layer and we must have more leniency towards the beginning.\n",
    "\tmodel.add(Dropout(0.2))\n",
    "    # The flatten layer flattens the weights from previous convolution layers before the layers become dense,\n",
    "    # this makes the values 1 dimensional and are therefor more easily passed to the first fully connected dense layer.\n",
    "\tmodel.add(Flatten())\n",
    "    # A dense layer of 128 neurons is added which also uses the relu activation\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "    # Our second dense layer is added with neurons correlated to the number of classes in our y output test group.\n",
    "    # Softmax activation is notably used here (as described above).\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# The model is compiled and we classify our loss, our optimizer (adamn, which is useful for stochastic gradients),\n",
    "    # and our metric which is accuracy and is used to examine performance.\n",
    "    # The categorical crossentropy is a useful loss alongside softmax activation, the loss function is ideal for one hot vectors.\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build, Fit, and Evaluate the model: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the code we are fitting the model and evaluating its performance for our analysis. In this section we will run the code and illustrate each of the 10 epochs for this particular network, it will show us the time it took for the epoch to run, the loss associated with that epoch, the accuracy as well as the loss and accuracy for the validation data set. For our data analysis we will also be examining the score of the DNN which is indicated by the Error as well as showcasing the loss and accuracy [loss,accuracy]; we will print a summary of our model indicating each layer, its output shape, and respective parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.4792 - accuracy: 0.8605 - val_loss: 0.0797 - val_accuracy: 0.9755\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0796 - accuracy: 0.9772 - val_loss: 0.0515 - val_accuracy: 0.9827\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0532 - accuracy: 0.9842 - val_loss: 0.0478 - val_accuracy: 0.9847\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0389 - accuracy: 0.9880 - val_loss: 0.0368 - val_accuracy: 0.9881\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0297 - accuracy: 0.9912 - val_loss: 0.0317 - val_accuracy: 0.9893\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 0.0261 - accuracy: 0.9915 - val_loss: 0.0307 - val_accuracy: 0.9896\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0198 - accuracy: 0.9942 - val_loss: 0.0325 - val_accuracy: 0.9888\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.0308 - val_accuracy: 0.9904\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.0278 - val_accuracy: 0.9898\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0286 - val_accuracy: 0.9905\n",
      "[0.02860221639275551, 0.9904999732971191]\n",
      "CNN Error: 0.95%\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 592,074\n",
      "Trainable params: 592,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reestablish our model from that which was defined above.\n",
    "model = baseline_model()\n",
    "# Fit the model, showcasing that our data is fit by the training sets input x and output y (training) and the validation set\n",
    "# the testing set, validation test for x and y. Epochs = number of passes through the DNN/CNN for training, batch size = sample size\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "# Evaluation of the Simple CNN, print score in the form of CNN error\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(scores)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "# Summarizes the model by layer, output shape, and parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare this Simple Neural Network to a Deep, more Complex One: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am showcasing a larger Convolution Neural Network that is deeper than the first simple layer, the deeper a network, the more layers that the network has. This is to showcase how we can improve the prediction capabilities of the network by adding more layers and increasing complexity to reduce loss, increase accuracy, and overall decrease percent error for the network. Notably, we added another Convolution 2D layer with a relu activation as well as a second dense layer but this time only 50 neurons. I will not be describing each line here, because the network is very similar, but I will annotate what layers were added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the Larger Convolution Neural Network model and architecture\n",
    "def larger_model():\n",
    "\t# Building the Architecture\n",
    "\tmodel2 = Sequential()\n",
    "\tmodel2.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\tmodel2.add(MaxPooling2D())\n",
    "    # Second Convolution layer is addedusing 15 convolution filters, and 3 kernel rows and columns, also using relu activation.\n",
    "\tmodel2.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    # Another filtering layer is added for reduced parameterization.\n",
    "\tmodel2.add(MaxPooling2D())\n",
    "\tmodel2.add(Dropout(0.2))\n",
    "\tmodel2.add(Flatten())\n",
    "\tmodel2.add(Dense(128, activation='relu'))\n",
    "    # Another dense layer is added this time with 50 neurons, still using a relu activation.\n",
    "\tmodel2.add(Dense(50, activation='relu'))\n",
    "\tmodel2.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Model is compiled with the same loss, optimization, and metric for evaluation.\n",
    "\tmodel2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.8186 - accuracy: 0.7424 - val_loss: 0.0837 - val_accuracy: 0.9736\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.1068 - accuracy: 0.9680 - val_loss: 0.0570 - val_accuracy: 0.9813\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0819 - accuracy: 0.9746 - val_loss: 0.0416 - val_accuracy: 0.9862\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 9s 29ms/step - loss: 0.0615 - accuracy: 0.9810 - val_loss: 0.0356 - val_accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.0348 - val_accuracy: 0.9882\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.0445 - accuracy: 0.9863 - val_loss: 0.0315 - val_accuracy: 0.9886\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 9s 29ms/step - loss: 0.0385 - accuracy: 0.9883 - val_loss: 0.0346 - val_accuracy: 0.9883\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.0347 - accuracy: 0.9896 - val_loss: 0.0284 - val_accuracy: 0.9901\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 9s 29ms/step - loss: 0.0315 - accuracy: 0.9897 - val_loss: 0.0292 - val_accuracy: 0.9898\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.0294 - accuracy: 0.9909 - val_loss: 0.0240 - val_accuracy: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c3b9d8e0d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the model\n",
    "model = larger_model()\n",
    "# Fit the model and run the epochs\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02396288700401783, 0.9926000237464905]\n",
      "Large CNN Error: 0.74%\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 30)        780       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 12, 12, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 10, 10, 15)        4065      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 15)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 5, 5, 15)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               48128     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 59,933\n",
      "Trainable params: 59,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation and New CNN error score\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(scores)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analysis of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The substance of our results come from the prediction accuracy and error that we observe through each of the epochs of the Neural Networks as well as their overall values. The results I see here prove my scientific hypothesis. A Deep Neural Network is able to analyze reconstructed images of babies' brains made from frequencies related to their specific mood and is able to accurately predict what part of the brain was reconstructed and can correlate that prediction to a specific mood. We can see that as the epochs pass, for both networks, the accuracy increases, and the loss decreases quite significantly. The simple neural network showcases an accuracy of 99.04% and a loss of 2.8%, with an error of 0.95%. So we can safely say that a strong majority of the time our model can look at the reconstructed images of the brains and determine what specific complex it is and determine what the mood is based on association. Furthermore, evaluating the larger Convolution Neural Network allows to see notable improvements, showcasing an accuracy of 99.3% a loss of 2.4%, with an overall error of just 0.74%. This indicates that by deepening our network and increasing its complexity we can improve our models prediction, though this seems slight, the closer we are able to get to perfect, the better; I would rather know more confidently what the babies mood is so that we can live in peace and understand with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources and help: machinelearning.com, elitedatascience.com , DataCamp, and Professor Schiffer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
